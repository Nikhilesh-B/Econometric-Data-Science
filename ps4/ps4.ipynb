{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b113467",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a9912",
   "metadata": {},
   "source": [
    "### 1a) False, 0.8 applies to the logistic cdf, and since it is non-linear the effect cannot just be estimated through the coefficient, one would have take the derivative of the logistic cdf. \n",
    "\n",
    "### 1b) Note below lowercase phi represents the pdf of the zero centered gaussian distribution with variance 1.\n",
    "\n",
    "$$ \\frac{dP}{d(age)} = \\frac{d(\\Phi(\\alpha+\\beta\\cdot\\text{age}+\\gamma\\text{age}^2+\\delta\\text{educ}))}{d(age)}$$\n",
    "\n",
    "$$ \\frac{dP}{d(age)} = \\frac{d(\\Phi(\\alpha+\\beta\\cdot\\text{age}+\\gamma\\text{age}^2+\\delta\\text{educ}))}{d(\\alpha+\\beta\\cdot\\text{age}+\\gamma\\text{age}^2+\\delta\\text{educ})} \\cdot \\frac{\\Phi(\\alpha+\\beta\\cdot\\text{age}+\\gamma\\text{age}^2+\\delta\\text{educ})}{d(age)}$$\n",
    "\n",
    "\n",
    "$$ \\frac{dP}{d(age)} = \\phi(\\alpha+\\beta\\cdot\\text{age}+\\gamma\\text{age}^2+\\delta\\text{educ}) \\cdot (\\beta \\cdot \\text{age} + 2\\cdot\\delta\\cdot \\text{age})$$\n",
    "\n",
    "\n",
    "### 1c) Yes potentially, you would be able to use this to determine whether we can utilize a distribution that favors the heavy tails more or not. If you have heavy tails then you may want to prefer using a normal cdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1e537",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49eeebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.discrete as smd\n",
    "\n",
    "df = pd.read_stata('JTRAIN2.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bef6c0",
   "metadata": {},
   "source": [
    "### 2a) Run a linear regression of train on several demographic and pretraining variables: unem74,unem75,age,educ,black,hisp,married. Are these variables jointly significant at the 5% level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46290495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>train</td>      <th>  R-squared:         </th> <td>   0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 03 Nov 2025</td> <th>  Prob (F-statistic):</th>  <td> 0.192</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:17:41</td>     <th>  Log-Likelihood:    </th> <td> -311.53</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   445</td>      <th>  AIC:               </th> <td>   639.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   437</td>      <th>  BIC:               </th> <td>   671.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>    0.3380</td> <td>    0.189</td> <td>    1.784</td> <td> 0.075</td> <td>   -0.034</td> <td>    0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>unem74</th>  <td>    0.0209</td> <td>    0.077</td> <td>    0.270</td> <td> 0.787</td> <td>   -0.131</td> <td>    0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>unem75</th>  <td>   -0.0956</td> <td>    0.072</td> <td>   -1.329</td> <td> 0.184</td> <td>   -0.237</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>     <td>    0.0032</td> <td>    0.003</td> <td>    0.942</td> <td> 0.347</td> <td>   -0.003</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>educ</th>    <td>    0.0120</td> <td>    0.013</td> <td>    0.900</td> <td> 0.368</td> <td>   -0.014</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>black</th>   <td>   -0.0817</td> <td>    0.088</td> <td>   -0.931</td> <td> 0.352</td> <td>   -0.254</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hisp</th>    <td>   -0.2000</td> <td>    0.117</td> <td>   -1.710</td> <td> 0.088</td> <td>   -0.430</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>married</th> <td>    0.0373</td> <td>    0.064</td> <td>    0.579</td> <td> 0.563</td> <td>   -0.089</td> <td>    0.164</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2209.423</td> <th>  Durbin-Watson:     </th> <td>   0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  68.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.327</td>  <th>  Prob(JB):          </th> <td>1.48e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 1.195</td>  <th>  Cond. No.          </th> <td>    250.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      train       & \\textbf{  R-squared:         } &     0.022   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.007   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     1.429   \\\\\n",
       "\\textbf{Date:}             & Mon, 03 Nov 2025 & \\textbf{  Prob (F-statistic):} &    0.192    \\\\\n",
       "\\textbf{Time:}             &     09:17:41     & \\textbf{  Log-Likelihood:    } &   -311.53   \\\\\n",
       "\\textbf{No. Observations:} &         445      & \\textbf{  AIC:               } &     639.1   \\\\\n",
       "\\textbf{Df Residuals:}     &         437      & \\textbf{  BIC:               } &     671.8   \\\\\n",
       "\\textbf{Df Model:}         &           7      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &       0.3380  &        0.189     &     1.784  &         0.075        &       -0.034    &        0.710     \\\\\n",
       "\\textbf{unem74}  &       0.0209  &        0.077     &     0.270  &         0.787        &       -0.131    &        0.173     \\\\\n",
       "\\textbf{unem75}  &      -0.0956  &        0.072     &    -1.329  &         0.184        &       -0.237    &        0.046     \\\\\n",
       "\\textbf{age}     &       0.0032  &        0.003     &     0.942  &         0.347        &       -0.003    &        0.010     \\\\\n",
       "\\textbf{educ}    &       0.0120  &        0.013     &     0.900  &         0.368        &       -0.014    &        0.038     \\\\\n",
       "\\textbf{black}   &      -0.0817  &        0.088     &    -0.931  &         0.352        &       -0.254    &        0.091     \\\\\n",
       "\\textbf{hisp}    &      -0.2000  &        0.117     &    -1.710  &         0.088        &       -0.430    &        0.030     \\\\\n",
       "\\textbf{married} &       0.0373  &        0.064     &     0.579  &         0.563        &       -0.089    &        0.164     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 2209.423 & \\textbf{  Durbin-Watson:     } &    0.035  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } &   68.291  \\\\\n",
       "\\textbf{Skew:}          &   0.327  & \\textbf{  Prob(JB):          } & 1.48e-15  \\\\\n",
       "\\textbf{Kurtosis:}      &   1.195  & \\textbf{  Cond. No.          } &     250.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  train   R-squared:                       0.022\n",
       "Model:                            OLS   Adj. R-squared:                  0.007\n",
       "Method:                 Least Squares   F-statistic:                     1.429\n",
       "Date:                Mon, 03 Nov 2025   Prob (F-statistic):              0.192\n",
       "Time:                        09:17:41   Log-Likelihood:                -311.53\n",
       "No. Observations:                 445   AIC:                             639.1\n",
       "Df Residuals:                     437   BIC:                             671.8\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3380      0.189      1.784      0.075      -0.034       0.710\n",
       "unem74         0.0209      0.077      0.270      0.787      -0.131       0.173\n",
       "unem75        -0.0956      0.072     -1.329      0.184      -0.237       0.046\n",
       "age            0.0032      0.003      0.942      0.347      -0.003       0.010\n",
       "educ           0.0120      0.013      0.900      0.368      -0.014       0.038\n",
       "black         -0.0817      0.088     -0.931      0.352      -0.254       0.091\n",
       "hisp          -0.2000      0.117     -1.710      0.088      -0.430       0.030\n",
       "married        0.0373      0.064      0.579      0.563      -0.089       0.164\n",
       "==============================================================================\n",
       "Omnibus:                     2209.423   Durbin-Watson:                   0.035\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               68.291\n",
       "Skew:                           0.327   Prob(JB):                     1.48e-15\n",
       "Kurtosis:                       1.195   Cond. No.                         250.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df['train']\n",
    "X = sm.add_constant(df[['unem74','unem75','age','educ','black','hisp','married']])\n",
    "\n",
    "model = sm.OLS(exog=X, endog=Y).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9556ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Model is not jointly signficant at 5 percent level since p-val is 0.192\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "print(\"X\"*100)\n",
    "print(\"Model is not jointly signficant at 5 percent level since p-val is 0.192\")\n",
    "print(\"X\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b658cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.667436\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>train</td>      <th>  No. Observations:  </th>  <td>   445</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>   437</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 03 Nov 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.01685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>09:17:41</td>     <th>  Log-Likelihood:    </th> <td> -297.01</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -302.10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.1785</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   -0.4241</td> <td>    0.487</td> <td>   -0.871</td> <td> 0.384</td> <td>   -1.379</td> <td>    0.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>unem74</th>  <td>    0.0530</td> <td>    0.199</td> <td>    0.266</td> <td> 0.790</td> <td>   -0.338</td> <td>    0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>unem75</th>  <td>   -0.2477</td> <td>    0.185</td> <td>   -1.339</td> <td> 0.181</td> <td>   -0.610</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>     <td>    0.0083</td> <td>    0.009</td> <td>    0.948</td> <td> 0.343</td> <td>   -0.009</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>educ</th>    <td>    0.0314</td> <td>    0.034</td> <td>    0.916</td> <td> 0.360</td> <td>   -0.036</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>black</th>   <td>   -0.2069</td> <td>    0.225</td> <td>   -0.920</td> <td> 0.358</td> <td>   -0.648</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hisp</th>    <td>   -0.5398</td> <td>    0.309</td> <td>   -1.750</td> <td> 0.080</td> <td>   -1.144</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>married</th> <td>    0.0966</td> <td>    0.166</td> <td>    0.584</td> <td> 0.560</td> <td>   -0.228</td> <td>    0.421</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &      train       & \\textbf{  No. Observations:  } &      445    \\\\\n",
       "\\textbf{Model:}           &      Probit      & \\textbf{  Df Residuals:      } &      437    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        7    \\\\\n",
       "\\textbf{Date:}            & Mon, 03 Nov 2025 & \\textbf{  Pseudo R-squ.:     } &  0.01685    \\\\\n",
       "\\textbf{Time:}            &     09:17:41     & \\textbf{  Log-Likelihood:    } &   -297.01   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -302.10   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } &   0.1785    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &      -0.4241  &        0.487     &    -0.871  &         0.384        &       -1.379    &        0.530     \\\\\n",
       "\\textbf{unem74}  &       0.0530  &        0.199     &     0.266  &         0.790        &       -0.338    &        0.444     \\\\\n",
       "\\textbf{unem75}  &      -0.2477  &        0.185     &    -1.339  &         0.181        &       -0.610    &        0.115     \\\\\n",
       "\\textbf{age}     &       0.0083  &        0.009     &     0.948  &         0.343        &       -0.009    &        0.026     \\\\\n",
       "\\textbf{educ}    &       0.0314  &        0.034     &     0.916  &         0.360        &       -0.036    &        0.099     \\\\\n",
       "\\textbf{black}   &      -0.2069  &        0.225     &    -0.920  &         0.358        &       -0.648    &        0.234     \\\\\n",
       "\\textbf{hisp}    &      -0.5398  &        0.309     &    -1.750  &         0.080        &       -1.144    &        0.065     \\\\\n",
       "\\textbf{married} &       0.0966  &        0.166     &     0.584  &         0.560        &       -0.228    &        0.421     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Probit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  train   No. Observations:                  445\n",
       "Model:                         Probit   Df Residuals:                      437\n",
       "Method:                           MLE   Df Model:                            7\n",
       "Date:                Mon, 03 Nov 2025   Pseudo R-squ.:                 0.01685\n",
       "Time:                        09:17:41   Log-Likelihood:                -297.01\n",
       "converged:                       True   LL-Null:                       -302.10\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1785\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.4241      0.487     -0.871      0.384      -1.379       0.530\n",
       "unem74         0.0530      0.199      0.266      0.790      -0.338       0.444\n",
       "unem75        -0.2477      0.185     -1.339      0.181      -0.610       0.115\n",
       "age            0.0083      0.009      0.948      0.343      -0.009       0.026\n",
       "educ           0.0314      0.034      0.916      0.360      -0.036       0.099\n",
       "black         -0.2069      0.225     -0.920      0.358      -0.648       0.234\n",
       "hisp          -0.5398      0.309     -1.750      0.080      -1.144       0.065\n",
       "married        0.0966      0.166      0.584      0.560      -0.228       0.421\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df['train']\n",
    "X = sm.add_constant(df[['unem74','unem75','age','educ','black','hisp','married']])\n",
    "\n",
    "probit_model = smd.discrete_model.Probit(exog=X, endog=Y).fit()\n",
    "probit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "042c7225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "The ratio test p-value is 0.1785 meaning that the model is not significant\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "print(\"X\"*100)\n",
    "print(f\"The ratio test p-value is 0.1785 meaning that the model is not significant\")\n",
    "print(\"X\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d4707d",
   "metadata": {},
   "source": [
    "### Based on your answers to parts (a) and (b), does it appear that participation in job training can b e treated as exogenous for explaining 1978 unemployment status? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce9e1",
   "metadata": {},
   "source": [
    "Yes it can be because the other variables aren't jointly significant (and therefore they are not considered a multivariate combination of previous data). It can be seen as exogenous for explaining 1978 unemployment status. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dcc980",
   "metadata": {},
   "source": [
    "### d) Simple OLS regression of unem78 train and report the results in equation form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5fea87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>unem78</td>      <th>  R-squared:         </th> <td>   0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   6.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 03 Nov 2025</td> <th>  Prob (F-statistic):</th>  <td>0.0127</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:17:41</td>     <th>  Log-Likelihood:    </th> <td> -284.30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   445</td>      <th>  AIC:               </th> <td>   572.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   443</td>      <th>  BIC:               </th> <td>   580.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3538</td> <td>    0.028</td> <td>   12.419</td> <td> 0.000</td> <td>    0.298</td> <td>    0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>train</th> <td>   -0.1106</td> <td>    0.044</td> <td>   -2.503</td> <td> 0.013</td> <td>   -0.197</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>529.053</td> <th>  Durbin-Watson:     </th> <td>   2.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  79.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.813</td>  <th>  Prob(JB):          </th> <td>6.51e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.727</td>  <th>  Cond. No.          </th> <td>    2.47</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      unem78      & \\textbf{  R-squared:         } &     0.014   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.012   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     6.265   \\\\\n",
       "\\textbf{Date:}             & Mon, 03 Nov 2025 & \\textbf{  Prob (F-statistic):} &   0.0127    \\\\\n",
       "\\textbf{Time:}             &     09:17:41     & \\textbf{  Log-Likelihood:    } &   -284.30   \\\\\n",
       "\\textbf{No. Observations:} &         445      & \\textbf{  AIC:               } &     572.6   \\\\\n",
       "\\textbf{Df Residuals:}     &         443      & \\textbf{  BIC:               } &     580.8   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       0.3538  &        0.028     &    12.419  &         0.000        &        0.298    &        0.410     \\\\\n",
       "\\textbf{train} &      -0.1106  &        0.044     &    -2.503  &         0.013        &       -0.197    &       -0.024     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 529.053 & \\textbf{  Durbin-Watson:     } &    2.008  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   79.145  \\\\\n",
       "\\textbf{Skew:}          &   0.813 & \\textbf{  Prob(JB):          } & 6.51e-18  \\\\\n",
       "\\textbf{Kurtosis:}      &   1.727 & \\textbf{  Cond. No.          } &     2.47  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 unem78   R-squared:                       0.014\n",
       "Model:                            OLS   Adj. R-squared:                  0.012\n",
       "Method:                 Least Squares   F-statistic:                     6.265\n",
       "Date:                Mon, 03 Nov 2025   Prob (F-statistic):             0.0127\n",
       "Time:                        09:17:41   Log-Likelihood:                -284.30\n",
       "No. Observations:                 445   AIC:                             572.6\n",
       "Df Residuals:                     443   BIC:                             580.8\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3538      0.028     12.419      0.000       0.298       0.410\n",
       "train         -0.1106      0.044     -2.503      0.013      -0.197      -0.024\n",
       "==============================================================================\n",
       "Omnibus:                      529.053   Durbin-Watson:                   2.008\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               79.145\n",
       "Skew:                           0.813   Prob(JB):                     6.51e-18\n",
       "Kurtosis:                       1.727   Cond. No.                         2.47\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df['unem78']\n",
    "X = sm.add_constant(df['train'])\n",
    "\n",
    "model = sm.OLS(exog=X, endog=Y).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ba8b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Since p-value on train is 0.013, this is very statistically signficiant.\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "print(\"X\"*100)\n",
    "print(\"Since p-value on train is 0.013, this is very statistically signficiant.\")\n",
    "print(\"X\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60930f6d",
   "metadata": {},
   "source": [
    "### e) Run a probit of unem78 on train and rep ort the results in equation form. Does it make sense to compare the probit coefficient on train with the coefficient obtained from the linear model in part (d)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5285d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.610298\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>unem78</td>      <th>  No. Observations:  </th>  <td>   445</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>   443</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 03 Nov 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.01147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>09:17:41</td>     <th>  Log-Likelihood:    </th> <td> -271.58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -274.73</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.01204</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -0.3750</td> <td>    0.080</td> <td>   -4.702</td> <td> 0.000</td> <td>   -0.531</td> <td>   -0.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>train</th> <td>   -0.3210</td> <td>    0.128</td> <td>   -2.498</td> <td> 0.012</td> <td>   -0.573</td> <td>   -0.069</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &      unem78      & \\textbf{  No. Observations:  } &      445    \\\\\n",
       "\\textbf{Model:}           &      Probit      & \\textbf{  Df Residuals:      } &      443    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        1    \\\\\n",
       "\\textbf{Date:}            & Mon, 03 Nov 2025 & \\textbf{  Pseudo R-squ.:     } &  0.01147    \\\\\n",
       "\\textbf{Time:}            &     09:17:41     & \\textbf{  Log-Likelihood:    } &   -271.58   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -274.73   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } &  0.01204    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      -0.3750  &        0.080     &    -4.702  &         0.000        &       -0.531    &       -0.219     \\\\\n",
       "\\textbf{train} &      -0.3210  &        0.128     &    -2.498  &         0.012        &       -0.573    &       -0.069     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Probit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                 unem78   No. Observations:                  445\n",
       "Model:                         Probit   Df Residuals:                      443\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Mon, 03 Nov 2025   Pseudo R-squ.:                 0.01147\n",
       "Time:                        09:17:41   Log-Likelihood:                -271.58\n",
       "converged:                       True   LL-Null:                       -274.73\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.01204\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.3750      0.080     -4.702      0.000      -0.531      -0.219\n",
       "train         -0.3210      0.128     -2.498      0.012      -0.573      -0.069\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df['unem78']\n",
    "X = sm.add_constant(df['train'])\n",
    "\n",
    "probit_model = smd.discrete_model.Probit(exog=X, endog=Y).fit()\n",
    "probit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2de427",
   "metadata": {},
   "source": [
    "You can only really compare the sign of the effects in the OLS regression and the probit model. Both show statisically significant relationships. It doesn't make sense to compare them in magnitude however"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d3f17",
   "metadata": {},
   "source": [
    "### (f) Find the fitted probabilities from parts (d) and (e). Explain why they are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b593f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are approximately the same\n",
      "\n",
      "First 10 OLS predictions: [0.24324324324324356, 0.24324324324324356, 0.24324324324324356, 0.24324324324324356, 0.24324324324324356, 0.24324324324324356, 0.24324324324324356, 0.24324324324324356, 0.24324324324324356, 0.24324324324324356]\n",
      "First 10 Probit predictions: [0.24324324324324326, 0.24324324324324326, 0.24324324324324326, 0.24324324324324326, 0.24324324324324326, 0.24324324324324326, 0.24324324324324326, 0.24324324324324326, 0.24324324324324326, 0.24324324324324326]\n",
      "\n",
      "Maximum absolute difference: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "OLS_preds = list(model.predict())\n",
    "probit_preds = list(probit_model.predict())\n",
    "\n",
    "# Check if predictions are approximately equal (within default tolerance)\n",
    "if np.allclose(OLS_preds, probit_preds):\n",
    "    print(\"They are approximately the same\")\n",
    "else:\n",
    "    print(\"Not the same\")\n",
    "\n",
    "# Show first 10 predictions from each model\n",
    "print(\"\\nFirst 10 OLS predictions:\", OLS_preds[:10])\n",
    "print(\"First 10 Probit predictions:\", probit_preds[:10])\n",
    "\n",
    "# Show maximum difference\n",
    "max_diff = np.max(np.abs(np.array(OLS_preds) - np.array(probit_preds)))\n",
    "print(f\"\\nMaximum absolute difference: {max_diff:.10f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1cf48",
   "metadata": {},
   "source": [
    "The predictions are the same because when you do univariate OLS and univariate probit model, the optimization must be the same numerically in terms of the maximum of likelihood at standard OLS procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706bc6d",
   "metadata": {},
   "source": [
    "## (g) Add all the controls from part (a) b oth to the linear regression and to the probit regression of unem78 on train. Are the fitted probabilities now identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a558bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OLS Model\n",
      "================================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 unem78   R-squared:                       0.046\n",
      "Model:                            OLS   Adj. R-squared:                  0.029\n",
      "Method:                 Least Squares   F-statistic:                     2.640\n",
      "Date:                Mon, 03 Nov 2025   Prob (F-statistic):            0.00780\n",
      "Time:                        09:17:41   Log-Likelihood:                -276.90\n",
      "No. Observations:                 445   AIC:                             571.8\n",
      "Df Residuals:                     436   BIC:                             608.7\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1632      0.176      0.927      0.355      -0.183       0.509\n",
      "train         -0.1117      0.044     -2.521      0.012      -0.199      -0.025\n",
      "unem74         0.0387      0.072      0.540      0.589      -0.102       0.179\n",
      "unem75         0.0160      0.067      0.239      0.811      -0.115       0.147\n",
      "age         4.332e-05      0.003      0.014      0.989      -0.006       0.006\n",
      "educ           0.0001      0.012      0.012      0.991      -0.024       0.024\n",
      "black          0.1888      0.081      2.322      0.021       0.029       0.349\n",
      "hisp          -0.0377      0.109     -0.347      0.729      -0.251       0.176\n",
      "married       -0.0254      0.060     -0.426      0.670      -0.143       0.092\n",
      "==============================================================================\n",
      "Omnibus:                      440.373   Durbin-Watson:                   2.004\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               70.714\n",
      "Skew:                           0.749   Prob(JB):                     4.41e-16\n",
      "Kurtosis:                       1.748   Cond. No.                         251.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591714\n",
      "         Iterations 5\n",
      "================================================================================\n",
      "Probit Model\n",
      "================================================================================\n",
      "                          Probit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 unem78   No. Observations:                  445\n",
      "Model:                         Probit   Df Residuals:                      436\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Mon, 03 Nov 2025   Pseudo R-squ.:                 0.04158\n",
      "Time:                        09:17:41   Log-Likelihood:                -263.31\n",
      "converged:                       True   LL-Null:                       -274.73\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.003570\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.0103      0.538     -1.878      0.060      -2.065       0.044\n",
      "train         -0.3366      0.132     -2.557      0.011      -0.595      -0.079\n",
      "unem74         0.1061      0.213      0.499      0.618      -0.311       0.523\n",
      "unem75         0.0636      0.197      0.323      0.747      -0.323       0.450\n",
      "age            0.0007      0.009      0.074      0.941      -0.017       0.019\n",
      "educ          -0.0019      0.037     -0.051      0.959      -0.074       0.070\n",
      "black          0.6337      0.274      2.310      0.021       0.096       1.171\n",
      "hisp          -0.1649      0.379     -0.435      0.663      -0.908       0.578\n",
      "married       -0.0778      0.177     -0.439      0.661      -0.425       0.269\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "Not the same\n",
      "\n",
      "First 10 OLS predictions: [0.27271825742439904, 0.07068344139995397, 0.2979965719925336, 0.29772237640252935, 0.29754955547604456, 0.2972173029454226, 0.2976933479322205, 0.2979389650170387, 0.29822699989451334, 0.0838564347934373]\n",
      "First 10 Probit predictions: [0.26857823653209445, 0.08942352721999941, 0.292542461326591, 0.2924958423282561, 0.29584836731645947, 0.2926348585488725, 0.2909187965580752, 0.2936580891182099, 0.28809976788650393, 0.10467007031452641]\n",
      "\n",
      "Maximum absolute difference: 0.0643153572\n",
      "\n",
      "================================================================================\n",
      "Answer: No, the fitted probabilities are NOT identical when we include\n",
      "multiple covariates. Unlike the univariate case in part (f), the OLS and\n",
      "probit models produce different predictions when controls are added.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# OLS Model: unem78 on train + all controls\n",
    "Y = df['unem78']\n",
    "X = sm.add_constant(df[['train', 'unem74','unem75','age','educ','black','hisp','married']])\n",
    "\n",
    "model_OLS = sm.OLS(endog=Y, exog=X).fit()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OLS Model\")\n",
    "print(\"=\"*80)\n",
    "print(model_OLS.summary())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Probit Model: unem78 on train + all controls\n",
    "Y = df['unem78']\n",
    "X = sm.add_constant(df[['train', 'unem74','unem75','age','educ','black','hisp','married']])\n",
    "\n",
    "probit_model = smd.discrete_model.Probit(endog=Y, exog=X).fit()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Probit Model\")\n",
    "print(\"=\"*80)\n",
    "print(probit_model.summary())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compare predictions\n",
    "OLS_preds = list(model_OLS.predict())\n",
    "probit_preds = list(probit_model.predict())\n",
    "\n",
    "# Check if predictions are approximately equal (within default tolerance)\n",
    "if np.allclose(OLS_preds, probit_preds):\n",
    "    print(\"They are approximately the same\")\n",
    "else:\n",
    "    print(\"Not the same\")\n",
    "\n",
    "# Show first 10 predictions from each model\n",
    "print(\"\\nFirst 10 OLS predictions:\", OLS_preds[:10])\n",
    "print(\"First 10 Probit predictions:\", probit_preds[:10])\n",
    "\n",
    "# Show maximum difference\n",
    "max_diff = np.max(np.abs(np.array(OLS_preds) - np.array(probit_preds)))\n",
    "print(f\"\\nMaximum absolute difference: {max_diff:.10f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Answer: No, the fitted probabilities are NOT identical when we include\")\n",
    "print(\"multiple covariates. Unlike the univariate case in part (f), the OLS and\")\n",
    "print(\"probit models produce different predictions when controls are added.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17a069",
   "metadata": {},
   "source": [
    "Clearly the predictions are not the same thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9dcb3c",
   "metadata": {},
   "source": [
    "### Calculate Average Partial Effects (APE) for both Linear and Probit Models\n",
    "\n",
    "We need to compute the APE for all variables in both models and compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6e4c6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Average Partial Effects - LINEAR MODEL (OLS)\n",
      "================================================================================\n",
      "\n",
      "For the linear probability model, the partial effect is constant and\n",
      "equals the coefficient for each variable:\n",
      "\n",
      "const      0.163182\n",
      "train     -0.111703\n",
      "unem74     0.038693\n",
      "unem75     0.015961\n",
      "age        0.000043\n",
      "educ       0.000144\n",
      "black      0.188833\n",
      "hisp      -0.037701\n",
      "married   -0.025437\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Average Partial Effects - PROBIT MODEL\n",
      "================================================================================\n",
      "\n",
      "For the probit model, APE = mean((X')) * _j\n",
      "Mean((X')) = 0.336235\n",
      "\n",
      "const     -0.339709\n",
      "train     -0.113173\n",
      "unem74     0.035673\n",
      "unem75     0.021389\n",
      "age        0.000227\n",
      "educ      -0.000636\n",
      "black      0.213062\n",
      "hisp      -0.055459\n",
      "married   -0.026148\n",
      "dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Average Partial Effects (APE) from the models in part (g)\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "# For Linear Model (OLS): APE = coefficient (constant across all observations)\n",
    "ape_ols = model_OLS.params\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Average Partial Effects - LINEAR MODEL (OLS)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFor the linear probability model, the partial effect is constant and\")\n",
    "print(\"equals the coefficient for each variable:\\n\")\n",
    "print(ape_ols)\n",
    "print(\"\\n\")\n",
    "\n",
    "# For Probit Model: APE = mean((X') * _j) for each variable j\n",
    "# where  is the standard normal PDF\n",
    "\n",
    "# Recreate X matrix for predictions\n",
    "X = sm.add_constant(df[['train', 'unem74','unem75','age','educ','black','hisp','married']])\n",
    "\n",
    "# Get the linear predictor X' for all observations\n",
    "X_beta = probit_model.predict(X, which='linear')\n",
    "\n",
    "# Calculate (X') - the standard normal PDF evaluated at X'\n",
    "phi_xbeta = norm.pdf(X_beta)\n",
    "\n",
    "# Calculate APE for each variable\n",
    "# APE_j = mean((X')) * _j\n",
    "mean_phi = phi_xbeta.mean()\n",
    "ape_probit = mean_phi * probit_model.params\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Average Partial Effects - PROBIT MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFor the probit model, APE = mean((X')) * _j\")\n",
    "print(f\"Mean((X')) = {mean_phi:.6f}\\n\")\n",
    "print(ape_probit)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc3bd610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: Average Partial Effects\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Variable  APE - Linear Model  APE - Probit Model  Ratio (Linear/Probit)\n",
      "   const            0.163182           -0.339709              -0.480359\n",
      "   train           -0.111703           -0.113173               0.987007\n",
      "  unem74            0.038693            0.035673               1.084660\n",
      "  unem75            0.015961            0.021389               0.746247\n",
      "     age            0.000043            0.000227               0.190653\n",
      "    educ            0.000144           -0.000636              -0.226791\n",
      "   black            0.188833            0.213062               0.886282\n",
      "    hisp           -0.037701           -0.055459               0.679801\n",
      " married           -0.025437           -0.026148               0.972808\n",
      "\n",
      "\n",
      "Scaling factor (mean (X')): 0.336235\n",
      "\n",
      "Note: The probit APE  Linear APE * 0.336235\n",
      "\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "Key observations:\n",
      "\n",
      "1. SIGN: Both models give the same sign for all variables (positive or negative effect)\n",
      "\n",
      "2. MAGNITUDE: The probit APE values are smaller in absolute value than the linear model\n",
      "   because they are scaled by (X'), which is approximately 0.3362 on average\n",
      "\n",
      "3. RELATIVE EFFECTS: The ratio of effects between variables is similar in both models\n",
      "\n",
      "4. For the LINEAR MODEL:\n",
      "   - The partial effect is CONSTANT across all observations\n",
      "   - APE_j = _j (the coefficient itself)\n",
      "\n",
      "5. For the PROBIT MODEL:\n",
      "   - The partial effect varies across observations\n",
      "   - APE_j = mean((X')) * _j\n",
      "   - (X') is the standard normal PDF evaluated at the linear predictor\n",
      "\n",
      "6. The probit model is more appropriate when we want to ensure predicted probabilities\n",
      "   stay between 0 and 1, while the linear model can produce predictions outside [0,1].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare APE from both models side by side\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Variable': ape_ols.index,\n",
    "    'APE - Linear Model': ape_ols.values,\n",
    "    'APE - Probit Model': ape_probit.values,\n",
    "    'Ratio (Linear/Probit)': ape_ols.values / ape_probit.values\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: Average Partial Effects\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Also show the scaling factor\n",
    "print(f\"Scaling factor (mean (X')): {mean_phi:.6f}\")\n",
    "print(f\"\\nNote: The probit APE  Linear APE * {mean_phi:.6f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Visual comparison (excluding intercept)\n",
    "print(\"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Key observations:\n",
    "\n",
    "1. SIGN: Both models give the same sign for all variables (positive or negative effect)\n",
    "\n",
    "2. MAGNITUDE: The probit APE values are smaller in absolute value than the linear model\n",
    "   because they are scaled by (X'), which is approximately {:.4f} on average\n",
    "\n",
    "3. RELATIVE EFFECTS: The ratio of effects between variables is similar in both models\n",
    "\n",
    "4. For the LINEAR MODEL:\n",
    "   - The partial effect is CONSTANT across all observations\n",
    "   - APE_j = _j (the coefficient itself)\n",
    "\n",
    "5. For the PROBIT MODEL:\n",
    "   - The partial effect varies across observations\n",
    "   - APE_j = mean((X')) * _j\n",
    "   - (X') is the standard normal PDF evaluated at the linear predictor\n",
    "\n",
    "6. The probit model is more appropriate when we want to ensure predicted probabilities\n",
    "   stay between 0 and 1, while the linear model can produce predictions outside [0,1].\n",
    "\"\"\".format(mean_phi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0d560eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AVERAGE PARTIAL EFFECTS COMPARISON (Excluding Intercept)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Variable  APE - Linear Model  APE - Probit Model  Difference\n",
      "   train           -0.111703           -0.113173    0.001470\n",
      "  unem74            0.038693            0.035673    0.003020\n",
      "  unem75            0.015961            0.021389   -0.005427\n",
      "     age            0.000043            0.000227   -0.000184\n",
      "    educ            0.000144           -0.000636    0.000780\n",
      "   black            0.188833            0.213062   -0.024229\n",
      "    hisp           -0.037701           -0.055459    0.017758\n",
      " married           -0.025437           -0.026148    0.000711\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Mean absolute difference: 0.006697\n",
      "Max absolute difference: 0.024229\n",
      "\n",
      "Average ratio (Linear/Probit): 0.6651\n",
      "This is approximately 1/(X') = 1/0.336235 = 2.9741\n"
     ]
    }
   ],
   "source": [
    "# Create a cleaner comparison table (excluding the intercept)\n",
    "\n",
    "comparison_clean = pd.DataFrame({\n",
    "    'Variable': ape_ols.index[1:],  # Exclude intercept\n",
    "    'APE - Linear Model': ape_ols.values[1:],\n",
    "    'APE - Probit Model': ape_probit.values[1:],\n",
    "    'Difference': ape_ols.values[1:] - ape_probit.values[1:]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AVERAGE PARTIAL EFFECTS COMPARISON (Excluding Intercept)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\")\n",
    "print(comparison_clean.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMean absolute difference: {np.abs(comparison_clean['Difference']).mean():.6f}\")\n",
    "print(f\"Max absolute difference: {np.abs(comparison_clean['Difference']).max():.6f}\")\n",
    "print(f\"\\nAverage ratio (Linear/Probit): {(ape_ols.values[1:] / ape_probit.values[1:]).mean():.4f}\")\n",
    "print(f\"This is approximately 1/(X') = 1/{mean_phi:.6f} = {1/mean_phi:.4f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Conclusion: Comparing Linear and Probit APE\n",
    "\n",
    "How do they compare?\n",
    "\n",
    "1. Direction of effects: Both models agree on the direction (sign) of the effect for each variable\n",
    "\n",
    "2. Magnitude: The probit APE values are consistently smaller in absolute value than the linear model APE because the probit APE is scaled by the standard normal PDF.\n",
    "\n",
    "3. Scaling relationship: The linear APE is approximately equal to Probit APE divided by the mean of the standard normal PDF evaluated at the linear predictor.\n",
    "\n",
    "4. Relative importance: The ranking of variables by importance is similar in both models.\n",
    "\n",
    "5. Interpretation: \n",
    "   - Linear model: A one-unit increase in a variable changes the probability by a constant amount (the coefficient) for all observations.\n",
    "   - Probit model: A one-unit increase in a variable changes the probability by an amount that varies across observations; the APE is the average of these individual effects.\n",
    "\n",
    "6. Which to use: The probit model is theoretically superior because it constrains predicted probabilities to [0,1], but the linear model is simpler for interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b4dd5",
   "metadata": {},
   "source": [
    "## (i) Re-estimate regression in (g) as logit, calculate its average partial effect and compare it with your results in (h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24b0d984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591959\n",
      "         Iterations 6\n",
      "================================================================================\n",
      "LOGIT MODEL\n",
      "================================================================================\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 unem78   No. Observations:                  445\n",
      "Model:                          Logit   Df Residuals:                      436\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Mon, 03 Nov 2025   Pseudo R-squ.:                 0.04118\n",
      "Time:                        09:17:41   Log-Likelihood:                -263.42\n",
      "converged:                       True   LL-Null:                       -274.73\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.003878\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.7073      0.911     -1.875      0.061      -3.492       0.077\n",
      "train         -0.5532      0.220     -2.516      0.012      -0.984      -0.122\n",
      "unem74         0.1958      0.352      0.556      0.578      -0.495       0.886\n",
      "unem75         0.0827      0.325      0.255      0.799      -0.553       0.719\n",
      "age            0.0004      0.015      0.024      0.981      -0.029       0.030\n",
      "educ          -0.0016      0.061     -0.026      0.979      -0.120       0.117\n",
      "black          1.1024      0.501      2.201      0.028       0.121       2.084\n",
      "hisp          -0.2436      0.694     -0.351      0.725      -1.603       1.116\n",
      "married       -0.1358      0.296     -0.458      0.647      -0.717       0.445\n",
      "==============================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logit Model: unem78 on train + all controls\n",
    "Y = df['unem78']\n",
    "X = sm.add_constant(df[['train', 'unem74','unem75','age','educ','black','hisp','married']])\n",
    "\n",
    "logit_model = smd.discrete_model.Logit(endog=Y, exog=X).fit()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOGIT MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(logit_model.summary())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d7a35aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Average Partial Effects - LOGIT MODEL\n",
      "================================================================================\n",
      "\n",
      "For the logit model, APE = mean((X') * (1 - (X'))) * _j\n",
      "Mean((X')) = Mean((X') * (1 - (X'))) = 0.203070\n",
      "\n",
      "const     -0.346708\n",
      "train     -0.112330\n",
      "unem74     0.039770\n",
      "unem75     0.016791\n",
      "age        0.000073\n",
      "educ      -0.000321\n",
      "black      0.223870\n",
      "hisp      -0.049476\n",
      "married   -0.027580\n",
      "dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Average Partial Effects (APE) for LOGIT Model\n",
    "\n",
    "# For Logit Model: APE = mean((X') * (1 - (X')) * _j) for each variable j\n",
    "# where  is the logistic CDF (which equals the predicted probability)\n",
    "\n",
    "# Get predicted probabilities P(Y=1|X) = (X')\n",
    "predicted_probs = logit_model.predict(X)\n",
    "\n",
    "# For logit, the marginal effect at each observation is: (X') * _j\n",
    "# where (X') = (X') * (1 - (X')) is the logistic PDF\n",
    "lambda_xbeta = predicted_probs * (1 - predicted_probs)\n",
    "\n",
    "# Calculate APE for each variable\n",
    "# APE_j = mean((X')) * _j\n",
    "mean_lambda = lambda_xbeta.mean()\n",
    "ape_logit = mean_lambda * logit_model.params\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Average Partial Effects - LOGIT MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFor the logit model, APE = mean((X') * (1 - (X'))) * _j\")\n",
    "print(f\"Mean((X')) = Mean((X') * (1 - (X'))) = {mean_lambda:.6f}\\n\")\n",
    "print(ape_logit)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2460fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: Average Partial Effects - All Three Models\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Variable  APE - Linear  APE - Probit  APE - Logit  Probit/Linear  Logit/Linear  Logit/Probit\n",
      "   const      0.163182     -0.339709    -0.346708      -2.081774     -2.124667      1.020604\n",
      "   train     -0.111703     -0.113173    -0.112330       1.013164      1.005616      0.992550\n",
      "  unem74      0.038693      0.035673     0.039770       0.921948      1.027856      1.114874\n",
      "  unem75      0.015961      0.021389     0.016791       1.340039      1.051992      0.785046\n",
      "     age      0.000043      0.000227     0.000073       5.245132      1.696755      0.323491\n",
      "    educ      0.000144     -0.000636    -0.000321      -4.409338     -2.228520      0.505409\n",
      "   black      0.188833      0.213062     0.223870       1.128308      1.185545      1.050728\n",
      "    hisp     -0.037701     -0.055459    -0.049476       1.471019      1.312332      0.892125\n",
      " married     -0.025437     -0.026148    -0.027580       1.027952      1.084238      1.054755\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SCALING FACTORS\n",
      "================================================================================\n",
      "Linear Model: APE = _j (constant partial effect)\n",
      "Probit Model: Mean((X')) = 0.336235\n",
      "Logit Model:  Mean((X')) = 0.203070\n",
      "\n",
      "Ratio: Logit/Probit scaling = 0.6040\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare APE from all three models: Linear, Probit, and Logit\n",
    "\n",
    "comparison_all = pd.DataFrame({\n",
    "    'Variable': ape_ols.index,\n",
    "    'APE - Linear': ape_ols.values,\n",
    "    'APE - Probit': ape_probit.values,\n",
    "    'APE - Logit': ape_logit.values,\n",
    "    'Probit/Linear': ape_probit.values / ape_ols.values,\n",
    "    'Logit/Linear': ape_logit.values / ape_ols.values,\n",
    "    'Logit/Probit': ape_logit.values / ape_probit.values\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: Average Partial Effects - All Three Models\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\")\n",
    "print(comparison_all.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SCALING FACTORS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Linear Model: APE = _j (constant partial effect)\")\n",
    "print(f\"Probit Model: Mean((X')) = {mean_phi:.6f}\")\n",
    "print(f\"Logit Model:  Mean((X')) = {mean_lambda:.6f}\")\n",
    "print(f\"\\nRatio: Logit/Probit scaling = {mean_lambda/mean_phi:.4f}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f293258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AVERAGE PARTIAL EFFECTS: Linear vs Probit vs Logit (Excluding Intercept)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Variable    Linear    Probit     Logit\n",
      "   train -0.111703 -0.113173 -0.112330\n",
      "  unem74  0.038693  0.035673  0.039770\n",
      "  unem75  0.015961  0.021389  0.016791\n",
      "     age  0.000043  0.000227  0.000073\n",
      "    educ  0.000144 -0.000636 -0.000321\n",
      "   black  0.188833  0.213062  0.223870\n",
      "    hisp -0.037701 -0.055459 -0.049476\n",
      " married -0.025437 -0.026148 -0.027580\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DIFFERENCES IN APE\n",
      "================================================================================\n",
      "\n",
      "Probit vs Linear:\n",
      "  Mean absolute difference: 0.006697\n",
      "  Max absolute difference: 0.024229\n",
      "\n",
      "Logit vs Linear:\n",
      "  Mean absolute difference: 0.006498\n",
      "  Max absolute difference: 0.035037\n",
      "\n",
      "Logit vs Probit:\n",
      "  Mean absolute difference: 0.003529\n",
      "  Max absolute difference: 0.010808\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a cleaner comparison (excluding the intercept) and visualize\n",
    "\n",
    "comparison_clean_all = pd.DataFrame({\n",
    "    'Variable': ape_ols.index[1:],  # Exclude intercept\n",
    "    'Linear': ape_ols.values[1:],\n",
    "    'Probit': ape_probit.values[1:],\n",
    "    'Logit': ape_logit.values[1:]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AVERAGE PARTIAL EFFECTS: Linear vs Probit vs Logit (Excluding Intercept)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\")\n",
    "print(comparison_clean_all.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Calculate differences\n",
    "print(\"=\"*80)\n",
    "print(\"DIFFERENCES IN APE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nProbit vs Linear:\")\n",
    "print(f\"  Mean absolute difference: {np.abs(ape_probit.values[1:] - ape_ols.values[1:]).mean():.6f}\")\n",
    "print(f\"  Max absolute difference: {np.abs(ape_probit.values[1:] - ape_ols.values[1:]).max():.6f}\")\n",
    "\n",
    "print(\"\\nLogit vs Linear:\")\n",
    "print(f\"  Mean absolute difference: {np.abs(ape_logit.values[1:] - ape_ols.values[1:]).mean():.6f}\")\n",
    "print(f\"  Max absolute difference: {np.abs(ape_logit.values[1:] - ape_ols.values[1:]).max():.6f}\")\n",
    "\n",
    "print(\"\\nLogit vs Probit:\")\n",
    "print(f\"  Mean absolute difference: {np.abs(ape_logit.values[1:] - ape_probit.values[1:]).mean():.6f}\")\n",
    "print(f\"  Max absolute difference: {np.abs(ape_logit.values[1:] - ape_probit.values[1:]).max():.6f}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae82bb",
   "metadata": {},
   "source": [
    "### Conclusion: Comparing Logit APE with Probit and Linear APE from part (h)\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Sign Consistency**: All three models (Linear, Probit, Logit) agree on the direction (sign) of the effect for each variable.\n",
    "\n",
    "2. **Magnitude Comparison**:\n",
    "   - Linear model APE values are typically the largest in absolute value\n",
    "   - Probit and Logit APE values are smaller because they're scaled by their respective PDFs\n",
    "   - Probit uses (X'), the standard normal PDF\n",
    "   - Logit uses (X') = (X')  (1 - (X')), the logistic PDF\n",
    "\n",
    "3. **Probit vs Logit**:\n",
    "   - The APE values from Probit and Logit are very similar\n",
    "   - The logit scaling factor is typically slightly larger than the probit scaling factor\n",
    "   - This is because the logistic distribution has heavier tails than the normal distribution\n",
    "   - In practice, the choice between probit and logit often makes little difference for APE\n",
    "\n",
    "4. **Formula Recap**:\n",
    "   - **Linear**: APE_j = _j (constant across all observations)\n",
    "   - **Probit**: APE_j = mean((X'))  _j, where  is the standard normal PDF\n",
    "   - **Logit**: APE_j = mean((X')  (1 - (X')))  _j, where  is the logistic CDF\n",
    "\n",
    "5. **Model Selection**:\n",
    "   - All three models give similar qualitative conclusions\n",
    "   - Probit and Logit are theoretically superior as they constrain predictions to [0,1]\n",
    "   - Linear is simpler to interpret but can give predictions outside [0,1]\n",
    "   - Choose between Probit/Logit based on assumptions about tail behavior\n",
    "\n",
    "6. **Variable of Interest (train)**:\n",
    "   - All three models show that training has a negative and statistically significant effect on 1978 unemployment\n",
    "   - The APE is approximately -0.11 to -0.11 across all three models, indicating very consistent results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c01075",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
